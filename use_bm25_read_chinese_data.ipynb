{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util import make_document\n",
    "\n",
    "DIR_PATH = \"chinese_data/\"\n",
    "\n",
    "documents = make_document(DIR_PATH)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\weiso131\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.453 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# 使用 jieba 對每篇文檔進行分詞\n",
    "tokenized_documents = [list(jieba.cut(doc)) for doc in documents]\n",
    "\n",
    "# 建立 BM25 模型\n",
    "bm25 = BM25Okapi(tokenized_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weiso131\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 加載模型和tokenizer\n",
    "model_name = \"fnlp/bart-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取出的關鍵詞: ['記憶源點', '為何', '功能']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'記憶源點 為何 功能 '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "query = \"記憶源點的功能為何\"\n",
    "\n",
    "# 使用 jieba 提取關鍵詞（例如提取前5個關鍵詞）\n",
    "keywords = jieba.analyse.extract_tags(query, topK=5)\n",
    "\n",
    "print(\"提取出的關鍵詞:\", keywords)\n",
    "\n",
    "keywords_query = \"\"\n",
    "for k in keywords:\n",
    "    keywords_query += k + \" \"\n",
    "keywords_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 相關文檔: 官方曾特別提醒玩家切勿購買第三方商家出售的記憶源點。  \n",
      "\n",
      "Top 2 相關文檔: 在遊戲內可用於解鎖已購買 /免費歌曲譜面。在移動版中也可用於購買世界模式\n",
      "體力，擴充好友數上限，或在 Legacy模式地圖 /Lost Chapter:Beyond/Breached \n",
      "Chapter 章節地圖獲得增益。在 Nintendo Switch 版中還可以用於升級搭檔。  \n",
      " 記憶源點  \n",
      "遊戲內付費道具，可以於官方網站購買，或 iOS版使用App Store 內購購買。\n",
      "部分情況下官方也可能會免費發放記憶源點。  \n",
      "\n",
      "Top 3 相關文檔: 在Lost Chapter:Beyond 和Breach Chapter 中，完全捨棄了其餘章節的階梯形\n",
      "式地圖，改用以百分數呈現的「 Beyond計量值」 。對於 Lost章節，該獎勵與遊\n",
      "玩結果，該地圖對選擇搭檔加成的「相性契合」值及搭檔的 OVER值有關；對\n",
      "於Breach章節，該獎勵與遊玩結果，每個地圖獨有的「鐫刻法則」有關，有時\n",
      "也和搭檔的 OVER值有關。玩家也可通過使用殘片，記憶源點和 /或Beyond強\n",
      "化計量條來提升遊玩加成。  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'官方曾特別提醒玩家切勿購買第三方商家出售的記憶源點。  \\n在遊戲內可用於解鎖已購買 /免費歌曲譜面。在移動版中也可用於購買世界模式\\n體力，擴充好友數上限，或在 Legacy模式地圖 /Lost Chapter:Beyond/Breached \\nChapter 章節地圖獲得增益。在 Nintendo Switch 版中還可以用於升級搭檔。  \\n 記憶源點  \\n遊戲內付費道具，可以於官方網站購買，或 iOS版使用App Store 內購購買。\\n部分情況下官方也可能會免費發放記憶源點。  \\n在Lost Chapter:Beyond 和Breach Chapter 中，完全捨棄了其餘章節的階梯形\\n式地圖，改用以百分數呈現的「 Beyond計量值」 。對於 Lost章節，該獎勵與遊\\n玩結果，該地圖對選擇搭檔加成的「相性契合」值及搭檔的 OVER值有關；對\\n於Breach章節，該獎勵與遊玩結果，每個地圖獨有的「鐫刻法則」有關，有時\\n也和搭檔的 OVER值有關。玩家也可通過使用殘片，記憶源點和 /或Beyond強\\n化計量條來提升遊玩加成。  \\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 使用 jieba 將查詢進行分詞\n",
    "tokenized_query = list(jieba.cut(keywords_query))\n",
    "\n",
    "# 計算每篇文檔的分數\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "top_n = bm25.get_top_n(tokenized_query, documents, n=3)\n",
    "retrieved_answers = \"\"\n",
    "# 列出前 3 篇最相關的文檔\n",
    "for idx, doc in enumerate(top_n):\n",
    "    print(f\"Top {idx + 1} 相關文檔: {doc}\")\n",
    "    retrieved_answers += doc\n",
    "\n",
    "retrieved_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本: 前 後 文 : 官 方 曾 特 別 提 醒 玩 家 切 勿 購 買 第 三 方 商 家 出 售 的 記 憶 源 點 可 以 在 遊 戲 內 可 用 於 解 鎖 已 購 買 / 免 費 歌\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 示例文本\n",
    "input_text = f\"前後文:{retrieved_answers}\\n\\n 提問:{query}\\n\\n 答案:\"\n",
    "\n",
    "# 將文本轉換為模型可讀的格式\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# 移除 'token_type_ids' (BART 不需要這個)\n",
    "inputs.pop('token_type_ids', None)\n",
    "# 生成結果\n",
    "outputs = model.generate(**inputs, max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "# 解碼生成的文本\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"生成的文本:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
